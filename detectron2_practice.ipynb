{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detectron2 for Computer Vision Image Recognition\n",
    "\n",
    "**Benedict Aryo**\n",
    "\n",
    "We will try to utilize latest package by Facebook AI Research [FAIR](https://ai.facebook.com/) from the creator of Mask-RCNN https://arxiv.org/abs/1703.06870 which aim to be the next-generation platform for object detection and segmentation.\n",
    "\n",
    "<br>\n",
    "\n",
    "___\n",
    "\n",
    "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"300\">\n",
    "\n",
    "Detectron2 is Facebook AI Research's next generation software system\n",
    "that implements state-of-the-art object detection algorithms.\n",
    "It is a ground-up rewrite of the previous version,\n",
    "[Detectron](https://github.com/facebookresearch/Detectron/),\n",
    "and it originates from [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark/). Detectron2 can be downloaded in: https://github.com/facebookresearch/detectron2\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://user-images.githubusercontent.com/1381301/66535560-d3422200-eace-11e9-9123-5535d469db19.png\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "## Detectron2 Benchmark\n",
    "Detectron2 having fastest training time compared with some other popular open source Mask R-CNN implementations. <br>\n",
    "**Here's the bechmarck:**\n",
    "\n",
    "<img src=\"https://github.com/BenedictusAryo/Mask-RCNN_Detectron2/raw/master/assets/detectron2_result.png\" width=\"400\">\n",
    "\n",
    "___\n",
    "## Detectron2 Model Zoo\n",
    "provide a large set of baseline results and trained models available for download in the [Detectron2 Model Zoo](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md).\n",
    "\n",
    "Detectron2 Pretrained model architecture can be used to:\n",
    "* Object Detection\n",
    "* Instance Segmentation\n",
    "* Panoptic Segmentation\n",
    "* Person Keypoint Detection\n",
    "* Semantic Segmentation (soon)\n",
    "\n",
    "___\n",
    "\n",
    "# Inference using Detectron2\n",
    "We will try using Detectron2 pretrained model to test it's prediction output while learning about it's functionality.\n",
    "\n",
    "See `detectron2` Documentation at: https://detectron2.readthedocs.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images to predict\n",
    "We need to prepare the sample input image to run prediction. In this case we will use sample image from [Ms. COCO Dataset](https://cocodataset.org/#home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download sample image using wget\n",
    "!wget http://images.cocodataset.org/val2017/000000439715.jpg -O input.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BGR test-image using opencv\n",
    "img = cv2.imread(\"./input.jpg\")\n",
    "\n",
    "# Show the RGB image using matplotlib imshow\n",
    "plt.imshow(img[...,::-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Mask R-CNN Instance Segmentation inference\n",
    "There are many implementation of Instance Segmentation provided by `detectron2`. \n",
    "\n",
    "Read about detectron2 model zoo here: <br> https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md\n",
    "\n",
    "For now, we will use the **`Mask R-CNN with ResNet-50 backbone and Feature Pyramid Network (FPN)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model we use is inside the 'COCO-InstanceSegmentation' directory, add to variable\n",
    "mrcnn_model = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image.\n",
    "\n",
    "**Note:** that if this is the first time you use this model, it will download the model weights file\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model Configuration object\n",
    "cfg = get_cfg()\n",
    "\n",
    "# Add model architecture config, which we set earlier\n",
    "cfg.merge_from_file(model_zoo.get_config_file(mrcnn_model))\n",
    "\n",
    "# Load the model weights that we set earlier\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(mrcnn_model)\n",
    "\n",
    "# Set threshold for this model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  \n",
    "\n",
    "# Create Predictor for inference based on configuration set\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "After setting the model configuration, Now we can simply use the predictor object to predict the Instance segmentation of input image\n",
    "\n",
    "**Note:** The Predictor is always take BGR image as the input (raw opencv img), so if previously you visualize using matplotlib (by changing to RGB, don't forget to change back to BGR)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the Instance segmentation of input image using predictor\n",
    "outputs = predictor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You can examine the output structure if you wish\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The Output of prediction will always in the `dictionary` format. <br>\n",
    "Also, since the model that we are using (**`Mask R-CNN`**) is in category **`Instance Segmentation`** so the output dictionary will have `instances` keys.\n",
    "\n",
    "For detail output format specification, See: https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "\n",
    "The output is inside the class attributes such as: \n",
    "* `.scores` for probability score\n",
    "* `.pred_classes` for classification output\n",
    "* `.pred_boxes` for bounding box detection\n",
    "* `.pred_masks` for instance segmentation\n",
    "\n",
    "\n",
    "And now, to Examine the inference result of this image we will print the class prediction and bounding box.\n",
    "\n",
    "**Note:** Some output tensor is still in `cuda` format tensor because we do inference using GPU.<br> It is identified at the end of the tensor by **`device='cuda:0'`** notation. <br>Before visualizing, we need to convert those tensor back to cpu, using `.to('cpu')` or convert the tensor to **list** using `.tolist()` so it can be visualized.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print how many object detected\n",
    "print('Object Detected: ', len(outputs['instances'].scores))\n",
    "\n",
    "# Print the Classification class\n",
    "print('Classification class: ', outputs['instances'].pred_classes.tolist())\n",
    "\n",
    "# Print bounding box for each predicted object\n",
    "print('Bounding Box: \\n', outputs['instances'].pred_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Since the model is trained using `COCO Datasets`, it has **`80 Class Object`** such as `person`, `dog`, `cat`, `car`, etc. More information please see: https://cocodataset.org/#home\n",
    "\n",
    "We can mapping the class number using `MetadataCatalog` which we already imported\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata namespace which refer to the Train Dataset in the configuration files\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
    "\n",
    "# Now we can check for example class `17` is what class\n",
    "print('Class 17 is ', metadata.thing_classes[17])\n",
    "\n",
    "# Now we can check for example class `0` is what class\n",
    "print('Class 0 is ', metadata.thing_classes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Visualize the output prediction\n",
    "Detectron2 also provide `Visualizer` module to draw the predictions on the image. the visualization is based on the `metadata` classes which we set previously.\n",
    "\n",
    "**Things to note:** `Visualizer()` function input is in `RGB` data, you can reverse the color channels using `[:, :, ::-1]` or using `cvt.cvtColor()` function from opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create viz object from `Visualizer`\n",
    "vis = Visualizer(img_rgb=img[...,::-1], metadata=metadata, scale=0.6) # Scale is the text size\n",
    "\n",
    "# Draw the prediction using `.draw_instance_prediction()` function\n",
    "out = vis.draw_instance_predictions(outputs['instances'].to('cpu'))\n",
    "\n",
    "# Show the image segmentation result\n",
    "plt.figure(figsize=(7,10))\n",
    "plt.imshow(out.get_image());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can also save the output image using\n",
    "```python\n",
    "out.save('filename')\n",
    "```\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output result to disk\n",
    "out.save('mrcnn_result.png')\n",
    "\n",
    "# Check whether it's exported\n",
    "!ls | grep mrcnn_result.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Other types of builtin models\n",
    "\n",
    "What if you want to do another types of computer vision detection, for example `human keypoints detection` ?\n",
    "\n",
    "How to do that in detectron2 ?<br>\n",
    "Simple, you only need to change the model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model to Human Keypoints detection\n",
    "keypoints_model = \"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The rests of the scrips is still the same as before\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(keypoints_model))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(keypoints_model)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9\n",
    "\n",
    "# Predict the keypoints result\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(img)\n",
    "\n",
    "# Print how many human keypoints detected\n",
    "print(\"Person keypoints detected: \", len(outputs['instances'].scores))\n",
    "\n",
    "# Get metadata for visualization\n",
    "metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
    "\n",
    "# Visualize the results\n",
    "vis = Visualizer(img_rgb=img[...,::-1], metadata=metadata, scale=1)\n",
    "out = vis.draw_instance_predictions(outputs['instances'].to('cpu'))\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.imshow(out.get_image());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "___\n",
    "\n",
    "# Training Mask R-CNN on a custom dataset <br> (Optional if still have time)\n",
    "This section is about using detectron2 to train Mask R-CNN with you own dataset. Detectron2 is providing simple API to train the models provided using our own image data.\n",
    "\n",
    "**Note:** That Mask R-CNN using COCO Dataset format, read more about the differences: https://medium.com/towards-artificial-intelligence/understanding-coco-and-pascal-voc-annotations-for-object-detection-bb8ffbbb36e3 \n",
    "\n",
    "For simplicity purposes, we will use [Fruits dataset](https://github.com/Tony607/detectron2_instance_segmentation_demo/releases/tag/V0.1) provided by [Chengwei Zhang](https://github.com/Tony607) which only has 18 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download & decompress the data (uncomment for download)\n",
    "# !wget https://github.com/Tony607/detectron2_instance_segmentation_demo/releases/download/V0.1/data.zip\n",
    "!unzip -o data.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "After Successfully download the balloon dataset, we can see the folder structure of the dataset using linux command `tree`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data\n",
    "!tree data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Since our dataset is already in COCO Dataset Format, you can see in above file that there's **`.json`** format, for example `trainval.json` that holds all image annotations of **class, bounding box,** and **instance mask.**\n",
    "\n",
    "So we can simply register the coco instances using  `register_coco_instances()` function from detectron2.\n",
    "\n",
    "**Note:** If your dataset format is in `VOC Pascal` you ca use function `register_pascal_voc()` from detectron2.data.datasets.pascal_voc\n",
    "\n",
    "<br>\n",
    "\n",
    "Then, to register the fruits_nuts dataset to detectron2, we will following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `register_coco_instances()` function\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# Register our coco dataset format by giving the directory folder\n",
    "register_coco_instances(\"fruits_nuts\", {}, \"./data/trainval.json\", \"./data/images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Remember:** you can't register the same dataset twice, for example if you run above cell again, It will throw **`AssertionError: Dataset 'fruits_nuts' is already registered!`**. <br>\n",
    "But don't worry if you want to clear those dataset, you can do it by:\n",
    "\n",
    "```python\n",
    "from detectron2.data.catalog import DatasetCatalog\n",
    "DatasetCatalog.clear()\n",
    "```\n",
    "\n",
    "After the coco format of the dataset is registered, now we can create the dataset & metadata catalog\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `MetadataCatalog` and `DatasetCatalog`\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "# Create Metadata & Dataset Catalog\n",
    "fruits_nuts_metadata = MetadataCatalog.get(\"fruits_nuts\")\n",
    "dataset_dicts = DatasetCatalog.get(\"fruits_nuts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "If success you will get notification like:\n",
    "\n",
    "```python\n",
    "Loaded ** images in COCO format from data/folder\n",
    "```\n",
    "\n",
    "Now, to verify the data loading is correct, let's visualize the annotations of randomly selected samples in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer\n",
    "import random\n",
    "\n",
    "\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], \n",
    "                            metadata=fruits_nuts_metadata, \n",
    "                            scale=0.4)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    plt.imshow(vis.get_image()[:, :, ::-1])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Train the Model !\n",
    "\n",
    "Now, let's fine tune a coco-pretrained R-50 FPN Mask R-CNN Model to our dataset.\n",
    "<br>\n",
    "We will use **`DefaultTrainer`** from `detectron2.engine` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import DefaultTrainer from engine & config function\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "# Mask R-CNN Model used\n",
    "mrcnn_model = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"\n",
    "\n",
    "# Set configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(mrcnn_model))\n",
    "cfg.DATASETS.TRAIN = (\"fruits_nuts\",)\n",
    "cfg.DATASETS.TEST = ()   # no metrics implemented for this dataset\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(mrcnn_model) # initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.02\n",
    "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough, but you can certainly train longer\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3  # 3 classes (data, fig, hazelnut)\n",
    "\n",
    "# Create output folder & Train\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Inference & Evaluation using trained model\n",
    "Now let's evaluate the training by inference it with the image, to do inference we must create a predictor using model that we just trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, 'model_final.pth')\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "cfg.DATASETS.TEST = (\"fruits_nuts\", )\n",
    "\n",
    "# Create Predictor for inference\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now we will use `predictor` to inference and visualize it's output\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "for d in random.sample(dataset_dicts, 3):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=fruits_nuts_metadata, \n",
    "                   scale=0.4,\n",
    "                   # remove the colors of unsegmented pixels\n",
    "                   instance_mode=ColorMode.IMAGE_BW   )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    plt.imshow(v.get_image()[:, :, ::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can also evaluate it's performance using AP metric implemented in COCO API\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator\n",
    "evaluator = COCOEvaluator(\"fruits_nuts\", cfg, False, output_dir=\"./output/\")\n",
    "trainer.test(cfg, trainer.model, evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Where to go from here\n",
    "You can test using different model or tuning the model parameter.\n",
    "\n",
    "To get hands on with another dataset, you can try [the balloon segmentation dataset](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon)\n",
    "which only has one class: balloon.\n",
    "\n",
    "simple download it by\n",
    "\n",
    "```python\n",
    "# download and decompress the data\n",
    "!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip\n",
    "!unzip balloon_dataset.zip\n",
    "```\n",
    "\n",
    "Or you can try some projects that using detectron2, for example [Point Rend](https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend)\n",
    "\n",
    "<img src=\"https://github.com/BenedictusAryo/Mask-RCNN_Detectron2/raw/master/assets/pointrend.png\" width=\"500\" align=\"center\">\n",
    "\n",
    "For another tutorial: https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
